{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c3062c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded. Shape: (2262314, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Load the clean dataset from your preprocessing notebook\n",
    "train_df = pd.read_csv(\"../data/processed/train_dataset.csv\")\n",
    "\n",
    "X_train = train_df.drop('Label', axis=1)\n",
    "y_train = train_df['Label']\n",
    "\n",
    "print(f\"Training data loaded. Shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd6d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# This dictionary holds all the models we'll train\n",
    "models = {\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=2000))\n",
    "    ]),\n",
    "    \"Linear SVM\": Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LinearSVC(dual=\"auto\", max_iter=5000, random_state=42))\n",
    "    ]),\n",
    "\"MLP Classifier\": Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', MLPClassifier(hidden_layer_sizes=(100,),max_iter=300,early_stopping=True,validation_fraction=0.1,n_iter_no_change=5,random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    # These models don't need scaling\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=100, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e009b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "--- Training Logistic Regression ---\n",
      "Saved trained model to: ../models/logistic_regression.pkl\n",
      "--- Training Linear SVM ---\n",
      "Saved trained model to: ../models/linear_svm.pkl\n",
      "--- Training MLP Classifier ---\n",
      "Saved trained model to: ../models/mlp_classifier.pkl\n",
      "--- Training Random Forest ---\n",
      "Saved trained model to: ../models/random_forest.pkl\n",
      "--- Training XGBoost ---\n",
      "Saved trained model to: ../models/xgboost.pkl\n",
      "--- Training LightGBM ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 445223, number of negative: 1817091\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6388\n",
      "[LightGBM] [Info] Number of data points in the train set: 2262314, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.196800 -> initscore=-1.406417\n",
      "[LightGBM] [Info] Start training from score -1.406417\n",
      "Saved trained model to: ../models/lightgbm.pkl\n",
      "\n",
      "All models have been trained and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting model training...\")\n",
    "\n",
    "output_dir = \"../models/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through the dictionary, train each model, and save it\n",
    "for name, model in models.items():\n",
    "    print(f\"--- Training {name} ---\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    filename = f\"{output_dir}{name.replace(' ', '_').lower()}.pkl\"\n",
    "    joblib.dump(model, filename)\n",
    "    \n",
    "    print(f\"Saved trained model to: {filename}\")\n",
    "\n",
    "print(\"\\nAll models have been trained and saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
